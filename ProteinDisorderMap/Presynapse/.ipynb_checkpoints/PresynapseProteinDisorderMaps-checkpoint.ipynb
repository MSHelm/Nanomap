{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presynapse Protein Disorder Maps\n",
    "\n",
    "Goal: Do the same analysis as for the postsynapse now with the presynapse data. If we find that the presynapse does not have specific regions of enriched/depeleted parameters, then the postsynapse is specific for that. We use the presynapse because we have the data and there is no other dataset available, that contains what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import pickle\n",
    "from scipy.io import loadmat, savemat\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "The presynapse data is not present as lego models, as we had for the postsynapse. Instead, I have a txt file for each protein with xyz coordinates in Angström of each copy. I need to bin it in 3D and then sum it up to create similar lego models. Silvio suggested 250 Angström bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '.\\\\coordinates_average_presynapse\\\\'\n",
    "files = glob.glob1(file_path, '*.txt')\n",
    "data = {file[:-4]: np.loadtxt(file_path + file) for file in files}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I do not know the full extent of the model, I need to create a complete dataset first to determine the bin edges. If I would run each protein individually, the bin edges would differ between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_bins(data, binsize_xy, binsize_z):\n",
    "    total_data = np.concatenate([data[x] for x in data], axis = 0)\n",
    "    minimums = total_data.min(axis = 0)\n",
    "    maximums = total_data.max(axis = 0)\n",
    "    minimums[0:2] /= binsize_xy #last value of range is excluded in python!\n",
    "    minimums[2] /= binsize_z\n",
    "    maximums[0:2] /= binsize_xy\n",
    "    maximums[2] /= binsize_z\n",
    "    minimums = np.floor(minimums)\n",
    "    maximums = np.ceil(maximums)\n",
    "    x_bins = np.arange(minimums[0] * binsize_xy, (maximums[0] + 1) * binsize_xy, binsize_xy) #add 1 to the maximum because this value is not included in the range\n",
    "    y_bins = np.arange(minimums[1] * binsize_xy, (maximums[1] + 1) * binsize_xy, binsize_xy)\n",
    "    z_bins = np.arange(minimums[2] * binsize_z, (maximums[2] + 1) * binsize_z, binsize_z)\n",
    "    return x_bins, y_bins, z_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each protein, bin the proteins according and put them into the dictionary copy_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bins, y_bins, z_bins = determine_bins(data, binsize_xy = 250, binsize_z = 1000)\n",
    "copy_maps = {key: np.histogramdd(values, [x_bins, y_bins, z_bins])[0] for key, values in data.items()} #the [0] indicator behind histogramdd ensures that we only pass the binned data, and ignore the binedges that are usually also returned.\n",
    "savemat('Presynapse_LegoModels.mat', copy_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary contains all presynapse proteins with the various identifiers. I use it to select and merge all the data from the different sources.\n",
    "protein_list = pd.read_excel('presynapse protein dictionary.xlsx')\n",
    "\n",
    "data_folder = Path('../')\n",
    "#table from Hanna Wildhagen with disorder scores and structure composition\n",
    "wildhagen_table = pd.read_excel(data_folder / \"table_structure_elements.xlsx\")\n",
    "wildhagen_table = wildhagen_table.drop(columns = ['Gene_Names', 'Protein_IDs', 'Protein_Names', 'Fasta_Headers', 'Isoelectric point EMBOSS', 'Isoelectric point DATASELECT'])\n",
    "df = pd.merge(protein_list, wildhagen_table, how = 'left', left_on = 'Mouse_Maj_Protein_Ids', right_on = 'Maj_Protein_IDs')\n",
    "df = df.drop(columns = 'Maj_Protein_IDs')\n",
    "\n",
    "#table from Cyriam et al.\n",
    "ciryam_table = pd.read_excel(data_folder / \"Ciryam_supp_table.xlsx\")\n",
    "ciryam_table = ciryam_table.rename(columns = ciryam_table.iloc[1])\n",
    "ciryam_table = ciryam_table[['Uniprot ID', 'Zagg', 'ZaggSC']]\n",
    "ciryam_table = ciryam_table.drop([0, 1])\n",
    "ciryam_table = ciryam_table.reset_index(drop = True)\n",
    "ciryam_table = ciryam_table.dropna()\n",
    "#select only proteins from human, we dont need C.elegans.\n",
    "ciryam_table = ciryam_table[ciryam_table['Uniprot ID'].str.contains('human')]\n",
    "df = pd.merge(df, ciryam_table, how = 'left', left_on = 'Human_entry_name', right_on = 'Uniprot ID').drop(columns = 'Uniprot ID')\n",
    "#add column with isoelectric point difference to neutral, because this makes the data interpretation easier later\n",
    "df['IsoelectricPointAverageDifferenceToNeutral'] = df['IsoelectricPointAverage'] - 7\n",
    "#reorder the dataframe\n",
    "df = df[['File_name', 'Length', 'Mass', 'DisorderLong', 'DisorderShort', 'IsoelectricPointAverage', 'IsoelectricPointAverageDifferenceToNeutral', 'Coil', 'ExtendedBetaSheet', 'AlphaHelix', 'StructuredRatio', 'Zagg', 'ZaggSC']]\n",
    "df = df.set_index('File_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Analysis\n",
    "\n",
    "In contrast to the postsynapse, I have all the lego models in the copy_maps dictionary this time. Again use the add_score function to deal with situations where no score is present in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_score(score_map, copy_map, score):\n",
    "    if isinstance(score, (int, float)) and ~np.isnan(score):\n",
    "        score_map += copy_map * score\n",
    "    return score_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the result maps\n",
    "score_maps = {score: np.zeros(copy_maps['actin'].shape) for score in ['Length', 'Mass', 'DisorderLong', 'DisorderShort', 'IsoelectricPointAverage', 'IsoelectricPointAverageDifferenceToNeutral', 'Coil', 'ExtendedBetaSheet', 'AlphaHelix', 'StructuredRatio', 'Zagg', 'ZaggSC']}\n",
    "copy_map_total = np.zeros(copy_maps['actin'].shape)\n",
    "\n",
    "for protein, copy_map in copy_maps.items():\n",
    "    copy_map_total += copy_map\n",
    "    scores = df.loc[protein, :]\n",
    "#     scores = scores[['Length', 'Mass', 'DisorderLong', 'DisorderShort', 'IsoelectricPointAverage', 'IsoelectricPointAverageDifferenceToNeutral', 'Coil', 'ExtendedBetaSheet', 'AlphaHelix', 'StructuredRatio', 'Zagg', 'ZaggSC']]\n",
    "    score_maps.update((score, add_score(score_map, copy_map, scores[score])) for score, score_map in score_maps.items())\n",
    "\n",
    "score_maps_orig = copy.deepcopy(score_maps) #Save a copy first for normalizations later on.\n",
    "\n",
    "#add the map with total copy numbers to the results for the simple analysis and save the data\n",
    "score_maps['CopyMap'] = copy_map_total \n",
    "with open('presynapse_scores.pkl', 'wb') as outfile:\n",
    "    pickle.dump(score_maps, outfile)\n",
    "savemat('presynapse_scores.mat', score_maps)\n",
    "\n",
    "# #Normalize data for protein concentration per voxel, to see whether the increase in aggregation prone proteins is independent of protein concentration.\n",
    "# score_maps = copy.deepcopy(score_maps_orig)\n",
    "# length_map = score_maps['Length']\n",
    "# length_map[length_map == 0] = 1  #to prevent division by 0\n",
    "# score_maps.update((score, score_map/length_map) for score, score_map in score_maps.items())\n",
    "# with open('presynapse_scores_normalized_aa.pkl', 'wb') as outfile:\n",
    "#     pickle.dump(score_maps, outfile)\n",
    "# savemat('presynapse_scores_normalized_aa.mat', score_maps)\n",
    "\n",
    "#Normalize data for protein copy number per voxel.\n",
    "score_maps = copy.deepcopy(score_maps_orig)\n",
    "copy_map_total[copy_map_total == 0] = 1 #to prevent division by 0.\n",
    "score_maps.update((score, score_map/copy_map_total) for score, score_map in score_maps.items())\n",
    "with open('presynapse_scores_normalized_copynum.pkl', 'wb') as outfile:\n",
    "    pickle.dump(score_maps, outfile)\n",
    "savemat('presynapse_scores_normalized_copynum.mat', score_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make heatmap plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programme\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Attempting to set identical bottom == top == 0.0 results in singular transformations; automatically expanding.\n",
      "D:\\Programme\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Attempting to set identical bottom == top == 0.0 results in singular transformations; automatically expanding.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "plt.ioff()\n",
    "\n",
    "files = glob.glob('*.pkl')\n",
    "for file in files:\n",
    "    with open(file, 'rb') as infile:\n",
    "        score_maps = pickle.load(infile)\n",
    "\n",
    "    with PdfPages(str(Path(file).with_suffix('')) + '.pdf') as pdf:\n",
    "        for score, score_map in score_maps.items():\n",
    "            fig = plt.figure()\n",
    "            plt.suptitle(score)\n",
    "            grid = ImageGrid(fig, 111, nrows_ncols = (4,3), share_all = True, cbar_location = 'right', cbar_mode = 'single')\n",
    "\n",
    "            #get max and min values to scale all images the same later on\n",
    "            vmin = np.min(score_map)\n",
    "            vmax = np.max(score_map)\n",
    "\n",
    "            for i, ax in enumerate(grid):\n",
    "                try:\n",
    "                    im = ax.imshow(score_map[:,:,i], cmap = 'inferno', vmin = vmin, vmax = vmax)\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "            ax.cax.colorbar(im)\n",
    "            ax.cax.toggle_label(True)            \n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Lego Model plots\n",
    "\n",
    "This is done to make the later investigation easier, which proteins might cause some regional score enrichments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "with PdfPages('Presynapse_LegoModels.pdf') as pdf:\n",
    "    for protein, copy_map in copy_maps.items():\n",
    "        fig = plt.figure()\n",
    "        plt.suptitle(protein)\n",
    "        grid = ImageGrid(fig, 111, nrows_ncols = (4,3), share_all = True, cbar_location = 'right', cbar_mode = 'single')\n",
    "        vmin = np.min(copy_map)\n",
    "        vmax = np.max(copy_map)\n",
    "        for i, ax in enumerate(grid):\n",
    "            try:\n",
    "                im = ax.imshow(copy_map[:,:,i], cmap = 'inferno', vmin = vmin, vmax = vmax)\n",
    "            except:\n",
    "                break\n",
    "                \n",
    "        ax.cax.colorbar(im)\n",
    "        ax.cax.toggle_label(True)\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
